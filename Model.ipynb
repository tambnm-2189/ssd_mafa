{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras.utils\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras import layers\n",
    "from keras import models\n",
    "\n",
    "import anchor as ac\n",
    "import utils\n",
    "import preprocessing as pp\n",
    "\n",
    "import importlib\n",
    "importlib.reload(ac)\n",
    "importlib.reload(pp)\n",
    "import os\n",
    "\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configuration\n",
    "# for anchors\n",
    "input_shape = (224, 224, 3) \n",
    "ss = [0.1, 0.2, 0.3]\n",
    "r = [1, 0.5, 1.5, 0.75]\n",
    "variance = np.array([0.1, 0.1, 0.2, 0.2])\n",
    "n_boxes = len(r)\n",
    "mode = 'training'\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "# for data_generator:\n",
    "mafa_paths = {i: os.path.join('data', i) for i in os.listdir('data')}\n",
    "\n",
    "# get path to train, test data\n",
    "train_images_path = mafa_paths['train_images']\n",
    "train_labels_path = os.path.join(mafa_paths['train_labels'], 'LabelTrainAll.mat')\n",
    "test_images_path = mafa_paths['test_images']\n",
    "test_labels_path = os.path.join(mafa_paths['test_labels'], 'LabelTestAll.mat')\n",
    "train_key = 'label_train'\n",
    "test_key = 'label_test'\n",
    "\n",
    "neg_iou_threshold = 0.1\n",
    "iou_threshold = 0.6\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### im using resnet 9 for base model of ssd\n",
    "\n",
    "def res_block(_input, filters, kernel_size ):\n",
    "    y = layers.Conv2D(filters= filters, kernel_size= kernel_size, padding= 'same')(_input)\n",
    "    y = layers.BatchNormalization()(y)\n",
    "    y = layers.Activation('relu')(y)\n",
    "    y = layers.Conv2D(filters= filters, kernel_size= kernel_size, padding= 'same')(y)\n",
    "    y = layers.BatchNormalization()(y)\n",
    "    _input = layers.Conv2D(filters= filters, kernel_size=1)(_input)\n",
    "    y = layers.add([_input, y])\n",
    "    y = layers.Activation('relu')(y)\n",
    "#     y = layers.SpatialDropout2D(dropout_rate)(y)\n",
    "    return y\n",
    "\n",
    "# consider to use spatial dropout\n",
    "input_data = layers.Input(shape= input_shape)\n",
    "x = keras.layers.ZeroPadding2D(padding=(3, 3))(input_data)\n",
    "x = layers.Conv2D(filters= 32, kernel_size= 7, strides= 2, padding= 'valid')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Activation('relu')(x)\n",
    "\n",
    "x = keras.layers.ZeroPadding2D(padding=(1, 1))(x)\n",
    "x = layers.MaxPool2D(strides= 2, pool_size= 3)(x)\n",
    "\n",
    "x1 = res_block(_input= x, filters= 64, kernel_size= 3)\n",
    "# x1 = layers.MaxPool2D(strides= 2)(x1)\n",
    "x2 = res_block(_input= x1, filters= 64, kernel_size= 3)\n",
    "x2 = layers.MaxPool2D(strides= 2)(x2)\n",
    "\n",
    "x3 = res_block(_input= x2, filters= 128, kernel_size= 3)\n",
    "x4 = res_block(_input= x3, filters= 128, kernel_size= 3)\n",
    "\n",
    "x5 = layers.Conv2D(filters= 64, kernel_size= 3, padding= 'same')(x4)\n",
    "x5 = layers.BatchNormalization()(x5)\n",
    "x5 = layers.Activation('relu')(x5)\n",
    "x5_1 = layers.MaxPool2D(strides= 2)(x5)\n",
    "\n",
    "x6 = layers.Conv2D(filters= 64, kernel_size= 3, padding= 'same')(x5_1)\n",
    "x6 = layers.BatchNormalization()(x6)\n",
    "x6 = layers.Activation('relu')(x6)\n",
    "x6_1 = layers.MaxPool2D(strides= 2)(x6)\n",
    "\n",
    "x7 = layers.Conv2D(filters= 64, kernel_size= 3, padding= 'same')(x6_1)\n",
    "x7 = layers.BatchNormalization()(x7)\n",
    "x7 = layers.Activation('relu')(x7)\n",
    "\n",
    "# output = layers.Reshape((-1, 2))(x7)\n",
    "# print(output)\n",
    "\n",
    "## bow build upon some predictor layers (b, total_boxes, 2 + 4)\n",
    "pretictor_layers = [x5, x6, x7]\n",
    "\n",
    "cls_preds = [layers.Conv2D(n_boxes*2, 3, padding= 'same')(x) for x in pretictor_layers ]\n",
    "# reshape each predict to(b, -1, 2)\n",
    "cls_preds = [layers.Reshape((-1, 2))(pred) for pred in cls_preds]\n",
    "# concat all cls pred (b, total_box, 2)\n",
    "cls_pred = layers.Concatenate(axis= 1)(cls_preds)\n",
    "\n",
    "reg_preds = [layers.Conv2D(n_boxes*4, 3, padding= 'same')(x) for x in pretictor_layers ]\n",
    "# reshape each predict to(b, -1, 2)\n",
    "reg_preds = [layers.Reshape((-1, 4))(pred) for pred in reg_preds]\n",
    "# concat all cls pred (b, total_box, 2)\n",
    "reg_pred = layers.Concatenate(axis= 1)(reg_preds)\n",
    "\n",
    "\n",
    "if mode == 'training':\n",
    "    #concat cls and boxes coordinates prediction\n",
    "    output = layers.Concatenate(axis= -1)([cls_pred, reg_pred])\n",
    "    \n",
    "elif mode == 'predicting':\n",
    "    \n",
    "    ### anchor boxes for inference (b, total_boxes, 8)\n",
    "    anchor_tensors = [ac.gen_anchor_tensor(x, s, r, variance) for x, s in zip(pretictor_layers,ss)]\n",
    "    anchor_tensor = layers.Concatenate(axis= 1)(anchor_tensors)\n",
    "    \n",
    "    # concat anchor tensor to prediction for inference purpose\n",
    "    output = layers.Concatenate(axis= -1)([cls_pred, reg_pred, anchor_tensor])\n",
    "else: \n",
    "    raise ValueError('not a valid mode')\n",
    "    \n",
    "model = models.Model(inputs = input_data, outputs = output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loss and compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(utils)\n",
    "loss = utils.LossSSD()\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(0.01), loss=loss.compute_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "### get fmap_shapes and anchor_boxes for labeling\n",
    "importlib.reload(ac)\n",
    "fmap_shapes = [(x._keras_shape[1], x._keras_shape[2]) for x in pretictor_layers]\n",
    "anchor_boxes = ac.all_anchor_boxes(fmap_shapes, ss, r)\n",
    "label_anchor = ac.LabelAnchor(anchor_boxes, variance, iou_threshold, neg_iou_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "importlib.reload(pp)\n",
    "train_generator = pp.DataGenerator(images_path = train_images_path, \n",
    "                                   labels_path = train_labels_path, \n",
    "                                   key = 'label_train',\n",
    "                                   label_anchor= label_anchor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(pp)\n",
    "test_generator = pp.DataGenerator(images_path = test_images_path, \n",
    "                                   labels_path = test_labels_path, \n",
    "                                   key = 'LabelTest',\n",
    "                                   label_anchor = label_anchor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set up and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-91d17f1a10cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m model.fit_generator(generator= train_generator, \n\u001b[0;32m----> 2\u001b[0;31m                     validation_data= test_generator)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitial_epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdo_validation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36m__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    366\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mnumber\u001b[0m \u001b[0mof\u001b[0m \u001b[0mbatches\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m         \"\"\"\n\u001b[0;32m--> 368\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "model.fit_generator(generator= train_generator, \n",
    "                    validation_data= test_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py(368)__len__()\n",
      "-> raise NotImplementedError\n",
      "(Pdb) l\n",
      "363  \t        \"\"\"Number of batch in the Sequence.\n",
      "364  \t\n",
      "365  \t        # Returns\n",
      "366  \t            The number of batches in the Sequence.\n",
      "367  \t        \"\"\"\n",
      "368  ->\t        raise NotImplementedError\n",
      "369  \t\n",
      "370  \t    def on_epoch_end(self):\n",
      "371  \t        \"\"\"Method called at the end of every epoch.\n",
      "372  \t        \"\"\"\n",
      "373  \t        pass\n",
      "--KeyboardInterrupt--\n",
      "(Pdb) c\n"
     ]
    }
   ],
   "source": [
    "import pdb\n",
    "pdb.pm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "configuration \n",
    "\n",
    "\n",
    "-target_size = 224\n",
    "\n",
    "- all coordinates \n",
    "+ are in normalize form of [0, 1] during computation\n",
    "+ absolute values in drawing\n",
    "\n",
    "+ corners: iou\n",
    "+ centroids: offset\n",
    "+ topleft : drawing\n",
    "\n",
    "- per image\n",
    "+ total_boxes \n",
    "+ n_boxes = len(r)*len(s) per layer\n",
    "\n",
    "- labeling anchor params: \n",
    "+ pos_iou_thres\n",
    "+ neg_iou_thres = beta*#n_pos_boxes or beta if n_pos_boxes = 0\n",
    "\n",
    "- inference: nms args:\n",
    "+ iou_thres\n",
    "+ score_thres\n",
    "+ top_k\n",
    "+ anchor_variance = [0.1, 0.1, 0.2, 0.2]\n",
    "\n",
    "- anchor generation:\n",
    "+ aspect ratios list\n",
    "+ scaling factor\n",
    "\n",
    "-loss\n",
    "+ alpha : weight of reg_loss"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "planing for coding ssds/ face detection/ mafa dataset\n",
    "\n",
    "input\n",
    "+ image (b, target_size, target_size, 3)\n",
    "+ label (b, total_boxes, 2 + 4)  \n",
    "total_boxes = all anchor boxes for 1 image, (pos, neg, neutral boxes)\n",
    "2:one-hot encoding (bg: [1, 0], pos [0, 1], neutral[0, 0])\n",
    "** Note: only take a fix number amount of negative (bg) boxes per image, set other boxes to neutral \n",
    "4: offsets (non_zero only for pos boxes, 0 for other boxes)\n",
    "\n",
    "note***\n",
    "\n",
    "\n",
    "\n",
    "model: (all in tensor)\n",
    "- input = Input(...)\n",
    "- base model -> 'conv_n' layer\n",
    "- predictor_layer: conv_n, conv_n+1, conv_n+2, conv_n+3 layers\n",
    "- anchor_tensors = [gen_anchor_tensor(conv, *args) for conv in conv_ns]  ## numpy (function)\n",
    "    + each image: n_boxes = fh*fw*len(r)*len(s)\n",
    "    + each anchor tensor (b, n_boxes, 4 + 4) 4: centroid coordinates, 4: variance\n",
    "    + tf.concat all anchor box (b, total_boxes, 8)\n",
    "- prediction for each predictor layer\n",
    "    + coords prediction\n",
    "        predicted_boxes = [conv2d(n_boxes*4, 3, pad = 'same')(conv) for conv in convs]\n",
    "        shape (b, fh, fw, n_boxes * 4)\n",
    "        reshape (b, -1, 4)\n",
    "        tf.concat all predicted_coords (b, total_boxes, 4)\n",
    "    + class prediction = [conv2d(n_boxes*2, 3, pad = 'same', activation = 'softmax')(conv) for conv in convs]\n",
    "        shape (b, fh, fw, n_boxes * 2)\n",
    "        reshape (b, -1, 2)\n",
    "        tf.concat all predicted_coords (b, total_boxes, 2)\n",
    "    + tf.concat all prediction predictions = (b, total_boxes, 2 + 4) 2 : confidence_score of bg and face\n",
    "        \n",
    "- if traning mode is \"training\" :\n",
    "    model = Model(in = input, out = predictions)\n",
    "    \n",
    "  else: \n",
    "    tf.concat anchor and predictions (b, total_boxes, 2 + 4 + 8)\n",
    "  \n",
    "\n",
    "    \n",
    "    \n",
    "Data_generator \n",
    "_ generate all anchor boxes for an image (total_boxes, 4) (dont include variance)\n",
    "    argument: shape of feature map predicted layers from model\n",
    "              {r}: same for all layers, {s} : 1 for each layer (predictor layer)\n",
    "\n",
    "- in _data_genrator : in numpy / per batch\n",
    "    + preprocess images\n",
    "    + resize true bb\n",
    "    + labeling anchor boxes , for each image (total_boxes, 2 + 4)\n",
    "        args: [true_bbs]\n",
    "              pos_iou_thres, neg_iou_thres\n",
    "              \n",
    "        return (b, total_boxes, 6)\n",
    "        \n",
    "- prediction (inference mode): in tensor\n",
    "    args: input (b, total_boxes, 2 + 4 + 8)\n",
    "          \n",
    "          score_thres\n",
    "          iou_thres\n",
    "          top_k boxes\n",
    "          \n",
    "    + convert from offset to centroids -> (ymin, xmin, ymax, xmax)\n",
    "    \n",
    "    + for each image: \n",
    "        + tf.nms -> m1 selected indices -> (b, m1, 4)\n",
    "        +  m1 can be smaller than top_k => pad 0\n",
    "        \n",
    "    + return tf.map_fn... -> (b, top_k, 4)\n",
    "    \n",
    "    \n",
    "- Loss: in tensor\n",
    "    + input: y_pred (b, total_boxes , 2 + 4)\n",
    "             y_true (b, total_boxes , 2 + 4)\n",
    "    + output : scalar loss for a whole batch, avarage on (batch size, # loss_item in reg_loss, and in cls loss)\n",
    "    \n",
    "    - class_loss: (b, total_boxes)\n",
    "       + only account for n_pos_boxes and m_neg_boxes:\n",
    "       + mask = not [0, 0] (b, total_boxes), #loss_boxes: (b,)\n",
    "       +sum(reg_loss * mask, axis = 1)/#loss_boxes\n",
    "    \n",
    "    \n",
    "    - regression_loss \n",
    "        + loss for every boxes (b, total_boxes)\n",
    "        + only account for reg_loss of true positive boxes\n",
    "            + mask (b, total_boxes) 1 for true pos box, 0 otherwise \n",
    "            + sum(reg_loss * mask, axis = 1)/# n_pos_boxes where n_pos_boxes : (b, )\n",
    "            \n",
    "    - total_loss = (class_loss + alpha*regression_loss)/b \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
